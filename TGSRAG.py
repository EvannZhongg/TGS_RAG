# TGSRAG.py

import sys
from pathlib import Path
import hashlib
import json
import yaml
import pandas as pd
import asyncio
import math

from pdf2md import process_pdf
from chunks import chunk_dispatcher
from embedding import generate_chunk_embeddings
from extraction import extract_entities_and_relations
from fusion import fuse_and_update_knowledge_base
from db_utils import DBManager


def main():
    print("ğŸš€ Starting TGSRAG processing pipeline (Stream Processing Mode)...")

    # åˆå§‹åŒ–æ€»tokenè®¡æ•°å™¨
    total_token_usage = {
        "embedding_chunks": 0,
        "extraction": 0,
        "embedding_entities": 0,
        "embedding_relations": 0
    }

    input_dir = Path("D:/Personal_Project/TSG_RAG/test")  # è¯·æ ¹æ®ä½ çš„å®é™…è·¯å¾„ä¿®æ”¹
    # ä¸´æ—¶ç›®å½•åªç”¨äºå­˜æ”¾ä¸­é—´è°ƒè¯•æ–‡ä»¶ï¼Œæ ¸å¿ƒæ•°æ®è¿›æ•°æ®åº“
    output_dir_base = Path("D:/Personal_Project/TSG_RAG/test_temp")
    config_path = Path("config.yaml")

    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
    except Exception as e:
        print(f"âŒ Error loading config: {e}")
        sys.exit(1)

    general_config = config.get('General', {})
    chunking_config = config.get('Chunking', {})
    embedding_config = config.get('Embedding', {})
    llm_config = config.get('LLM', {})
    extraction_config = config.get('Extraction', {})

    rag_space = general_config.get('rag_space')
    rag_space_path = Path(rag_space)

    # è·å– Batch Size é…ç½®
    # embedding çš„ batch size é€šå¸¸è¾ƒå¤§ï¼ˆå¦‚ 10-20ï¼‰ï¼ŒLLM å¹¶å‘é€šå¸¸è¾ƒå°ï¼ˆå¦‚ 4-5ï¼‰
    # æˆ‘ä»¬å– embedding çš„ max_batch_size ä½œä¸ºå¤„ç†å•å…ƒï¼Œæˆ–è€…è‡ªå®šä¹‰ä¸€ä¸ªæµæ°´çº¿ batch
    pipeline_batch_size = embedding_config.get('max_batch_size', 10)
    print(f"âš™ï¸ Pipeline Batch Size set to: {pipeline_batch_size}")

    # åˆå§‹åŒ– DBManager æ£€æŸ¥å·²å¤„ç†æ–‡æ¡£
    db_manager = DBManager()
    existing_chunks_df = db_manager.load_df('chunks')
    processed_docs = set(existing_chunks_df['source_document_name'].unique()) if not existing_chunks_df.empty else set()

    input_dir.mkdir(exist_ok=True)
    output_dir_base.mkdir(exist_ok=True)

    for doc_path in input_dir.iterdir():
        if not doc_path.is_file():
            continue

        print(f"\n{'=' * 50}\n-> Found document: {doc_path.name}")

        if doc_path.name in processed_docs:
            print(f"âœ… Document '{doc_path.name}' has been processed before (found in DB). Skipping.")
            continue

        doc_hash_name = hashlib.md5(doc_path.read_bytes()).hexdigest()
        unique_output_dir = output_dir_base / doc_hash_name
        unique_output_dir.mkdir(parents=True, exist_ok=True)

        # 1. è¯»å–ä¸è½¬æ¢
        text_to_chunk = ""
        if doc_path.suffix.lower() == '.pdf':
            md_path = process_pdf(doc_path, unique_output_dir)
            if md_path and md_path.is_file():
                text_to_chunk = md_path.read_text(encoding='utf-8')
        elif doc_path.suffix.lower() in ['.txt', '.md']:
            text_to_chunk = doc_path.read_text(encoding='utf-8')
        else:
            print(f"â„¹ï¸  Skipping: Unsupported file type")
            continue

        if text_to_chunk:
            # 2. åˆ†å— (Chunking)
            print("ğŸ§  Chunking text...")
            all_chunks = chunk_dispatcher(text_to_chunk, doc_hash_name, chunking_config)
            source_filename = doc_path.name
            for chunk in all_chunks:
                chunk['source_document_name'] = source_filename

            total_chunks = len(all_chunks)
            print(f"ğŸ“¦ Total chunks generated: {total_chunks}")

            # 3. åˆ†æ‰¹å¤„ç†æµæ°´çº¿ (Embedding -> Extraction -> Fusion/Save)
            # ä½¿ç”¨ range å’Œåˆ‡ç‰‡è¿›è¡Œæ‰¹å¤„ç†
            num_batches = math.ceil(total_chunks / pipeline_batch_size)

            for i in range(0, total_chunks, pipeline_batch_size):
                current_batch_idx = i // pipeline_batch_size + 1
                batch_chunks = all_chunks[i: i + pipeline_batch_size]

                print(
                    f"\nğŸ”„ Processing Batch {current_batch_idx}/{num_batches} (Chunks {i + 1}-{min(i + pipeline_batch_size, total_chunks)})...")

                # 3.1 Batch Embedding (Chunks)
                # å³æ—¶è°ƒç”¨ Embedding
                if embedding_config.get('api_key'):
                    batch_chunks_embedded, tokens = generate_chunk_embeddings(batch_chunks, embedding_config)
                    total_token_usage["embedding_chunks"] += tokens
                    # è¿‡æ»¤æ‰ embedding å¤±è´¥çš„
                    batch_chunks_ready = [c for c in batch_chunks_embedded if c.get('embedding') is not None]
                else:
                    batch_chunks_ready = batch_chunks

                if not batch_chunks_ready:
                    print("âš ï¸ Batch skipped due to embedding failure.")
                    continue

                # 3.2 Batch Extraction (LLM)
                if llm_config.get('api_key'):
                    print(f"âœ¨ Extracting entities from batch...")
                    batch_entities, batch_relations, tokens = asyncio.run(extract_entities_and_relations(
                        batch_chunks_ready, llm_config, extraction_config
                    ))
                    total_token_usage["extraction"] += tokens

                    # 3.3 Batch Fusion & Save (è¿™æ˜¯å…³é”®ï¼Œæ¯æ‰¹å¤„ç†å®Œç«‹å³å­˜åº“)
                    # fuse_and_update_knowledge_base å†…éƒ¨ä¼šå¤„ç†ï¼š
                    # 1. å®ä½“/å…³ç³»å»é‡
                    # 2. è°ƒç”¨ generate_entity_embeddings / generate_relation_embeddings è¡¥å……ç¼ºå¤±çš„å‘é‡
                    # 3. ä¿å­˜åˆ°æ•°æ®åº“
                    if batch_entities or batch_relations or batch_chunks_ready:
                        doc_chunks_df = pd.DataFrame(batch_chunks_ready)
                        fuse_and_update_knowledge_base(
                            batch_entities,
                            batch_relations,
                            doc_chunks_df,
                            rag_space_path,
                            embedding_config,
                            llm_config,
                            total_token_usage
                        )
                    else:
                        print("â„¹ï¸  Batch yielded no new knowledge.")
                else:
                    # å¦‚æœæ²¡æœ‰ LLM Keyï¼Œè‡³å°‘ä¿å­˜ Chunk åˆ°æ•°æ®åº“
                    print("ğŸ’¾ Saving chunks only (No Extraction)...")
                    doc_chunks_df = pd.DataFrame(batch_chunks_ready)
                    fuse_and_update_knowledge_base(
                        [], [], doc_chunks_df, rag_space_path, embedding_config, total_token_usage
                    )

    # --- æœ€ç»ˆæŠ¥å‘Š ---
    print(f"\n{'=' * 50}\nğŸ“Š Total Token Usage Report ğŸ“Š")
    grand_total = 0
    for key, value in total_token_usage.items():
        print(f"   - {key.replace('_', ' ').title()}: {value:,} tokens")
        grand_total += value
    print(f"   ------------------------------------")
    print(f"   - Grand Total: {grand_total:,} tokens")
    print(f"{'=' * 50}")

    print(f"\nğŸ Processing pipeline finished. ğŸ")


if __name__ == "__main__":
    main()