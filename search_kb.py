import argparse
import yaml
import pandas as pd
import json
from sqlalchemy import text
from db_utils import DBManager
import sys


def list_knowledge_bases(config_path='config.yaml'):
    """åˆ—å‡ºå½“å‰æ•°æ®åº“ä¸­çš„æ‰€æœ‰çŸ¥è¯†åº“ (Schemas)"""
    print(f"ğŸ” æ­£åœ¨è¯»å–æ•°æ®åº“é…ç½®: {config_path}...")
    try:
        # ä¸´æ—¶åˆå§‹åŒ–ä»¥è·å–è¿æ¥å¼•æ“
        db = DBManager(config_path)
        engine = db.get_engine()

        # æŸ¥è¯¢æ‰€æœ‰éç³»ç»Ÿ Schema
        sql = text("""
            SELECT schema_name 
            FROM information_schema.schemata 
            WHERE schema_name NOT IN ('information_schema', 'public') 
              AND schema_name NOT LIKE 'pg_%'
        """)

        with engine.connect() as conn:
            result = conn.execute(sql)
            schemas = [row[0] for row in result]

        print("\nğŸ“š å½“å‰å­˜åœ¨çš„çŸ¥è¯†åº“ (Rag Spaces):")
        print("=" * 40)
        if schemas:
            for s in schemas:
                print(f"  â€¢ {s}")
        else:
            print("  (æš‚æ— è‡ªå®šä¹‰çŸ¥è¯†åº“)")
        print("=" * 40)

    except Exception as e:
        print(f"âŒ è·å–çŸ¥è¯†åº“åˆ—è¡¨å¤±è´¥: {e}")


def _batch_fetch_chunks(engine, schema, chunk_ids):
    """æ‰¹é‡è·å– Chunk è¯¦æƒ…"""
    if not chunk_ids:
        return {}

    # å»é‡å¹¶è½¬ tuple
    ids = tuple(set(chunk_ids))
    if not ids: return {}

    ids_sql = str(ids)
    if len(ids) == 1: ids_sql = f"('{ids[0]}')"

    sql = text(f"SELECT chunk_id, source_document_name, text FROM {schema}.chunks WHERE chunk_id IN {ids_sql}")

    chunk_map = {}
    try:
        with engine.connect() as conn:
            df = pd.read_sql(sql, conn)
            for _, row in df.iterrows():
                chunk_map[row['chunk_id']] = {
                    'doc': row['source_document_name'],
                    'preview': row['text'][:50].replace('\n', ' ') + "..."
                }
    except Exception as e:
        print(f"  âš ï¸ Chunk è¯¦æƒ…è·å–å¤±è´¥: {e}")

    return chunk_map


def _batch_fetch_entity_names(engine, schema, entity_ids):
    """æ‰¹é‡è·å–å®ä½“åç§°"""
    if not entity_ids:
        return {}

    ids = tuple(set(entity_ids))
    if not ids: return {}

    ids_sql = str(ids)
    if len(ids) == 1: ids_sql = f"('{ids[0]}')"

    sql = text(f"SELECT entity_id, entity_name FROM {schema}.entities WHERE entity_id IN {ids_sql}")

    name_map = {}
    try:
        with engine.connect() as conn:
            df = pd.read_sql(sql, conn)
            for _, row in df.iterrows():
                name_map[row['entity_id']] = row['entity_name']
    except Exception as e:
        pass
    return name_map


def search_knowledge_base(query_str, scope, config_path='config.yaml'):
    try:
        db = DBManager(config_path)
        engine = db.get_engine()
        schema = db.schema
    except Exception as e:
        print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        return

    print(f"ğŸ” æœç´¢ç›®æ ‡: '{query_str}' | çŸ¥è¯†åº“: {db.rag_space} ({schema})")
    print("=" * 80)

    search_pattern = f"%{query_str}%"

    # --- 1. æœç´¢å®ä½“ ---
    if 'entities' in scope:
        print(f"\n[ğŸ§© Entities / å®ä½“]")
        sql = text(f"""
            SELECT * FROM {schema}.entities 
            WHERE entity_name ILIKE :pattern OR description ILIKE :pattern
            LIMIT 20
        """)

        with engine.connect() as conn:
            df = pd.read_sql(sql, conn, params={"pattern": search_pattern})

        if not df.empty:
            # é¢„å¤„ç† chunk ids
            all_chunk_ids = []
            for x in df['source_chunk_ids']:
                if isinstance(x, list):
                    all_chunk_ids.extend(x)
                elif isinstance(x, str):
                    all_chunk_ids.extend(json.loads(x))

            chunk_map = _batch_fetch_chunks(engine, schema, all_chunk_ids)

            for _, row in df.iterrows():
                print(f"  ğŸ“ {row['entity_name']} (Type: {row['entity_type']})")
                print(f"     ID: {row['entity_id']}")
                print(f"     ğŸ“Š æƒé‡(Freq): {row['frequency']} | è¿æ¥åº¦(Degree): {row['degree']}")

                desc = row['description']
                if len(desc) > 100: desc = desc[:100] + "..."
                print(f"     ğŸ“ æè¿°: {desc}")

                # æ˜¾ç¤ºæ¥æº
                src_ids = row['source_chunk_ids']
                if isinstance(src_ids, str): src_ids = json.loads(src_ids)

                if src_ids:
                    print(f"     ğŸ“„ æ¥æº ({len(src_ids)} Chunks):")
                    # æŒ‰æ–‡æ¡£èšåˆæ˜¾ç¤º
                    docs = {}
                    for cid in src_ids:
                        info = chunk_map.get(cid, {'doc': 'Unknown', 'preview': '?'})
                        dname = info['doc']
                        if dname not in docs: docs[dname] = []
                        docs[dname].append(cid)

                    for dname, cids in docs.items():
                        print(f"       - æ–‡æ¡£: {dname}")
                        print(f"         Chunks: {', '.join(cids)}")
                print("    " + "-" * 60)
            print(f"  âœ… æ‰¾åˆ° {len(df)} ä¸ªç›¸å…³å®ä½“ã€‚")
        else:
            print("  (æœªæ‰¾åˆ°ç›¸å…³å®ä½“)")

    # --- 2. æœç´¢å…³ç³» ---
    if 'relations' in scope:
        print(f"\n[ğŸ”— Relationships / å…³ç³»]")
        sql = text(f"""
            SELECT * FROM {schema}.relationships 
            WHERE source_name ILIKE :pattern 
               OR target_name ILIKE :pattern 
               OR keywords ILIKE :pattern
            LIMIT 20
        """)

        with engine.connect() as conn:
            df = pd.read_sql(sql, conn, params={"pattern": search_pattern})

        if not df.empty:
            all_chunk_ids = []
            for x in df['source_chunk_ids']:
                if isinstance(x, list):
                    all_chunk_ids.extend(x)
                elif isinstance(x, str):
                    all_chunk_ids.extend(json.loads(x))
            chunk_map = _batch_fetch_chunks(engine, schema, all_chunk_ids)

            for _, row in df.iterrows():
                print(f"  ğŸ”— {row['source_name']} -> {row['target_name']}")
                print(f"     ID: {row['relation_id']}")
                print(f"     ğŸ·ï¸  å…³é”®è¯: {row['keywords']}")
                print(f"     ğŸ“Š æƒé‡(Freq): {row['frequency']} | è¿æ¥åº¦(Degree): {row['degree']}")
                print(f"     ğŸ“ æè¿°: {row['description'][:100]}..." if len(
                    str(row['description'])) > 100 else f"     ğŸ“ æè¿°: {row['description']}")

                src_ids = row['source_chunk_ids']
                if isinstance(src_ids, str): src_ids = json.loads(src_ids)

                if src_ids:
                    print(f"     ğŸ“„ æ¥æº ({len(src_ids)} Chunks):")
                    docs = {}
                    for cid in src_ids:
                        info = chunk_map.get(cid, {'doc': 'Unknown'})
                        dname = info['doc']
                        if dname not in docs: docs[dname] = []
                        docs[dname].append(cid)
                    for dname, cids in docs.items():
                        print(f"       - {dname}: {', '.join(cids)}")
                print("    " + "-" * 60)
            print(f"  âœ… æ‰¾åˆ° {len(df)} ä¸ªç›¸å…³å…³ç³»ã€‚")
        else:
            print("  (æœªæ‰¾åˆ°ç›¸å…³å…³ç³»)")

    # --- 3. æœç´¢æ–‡æœ¬å— ---
    if 'chunks' in scope:
        print(f"\n[ğŸ“„ Chunks / æ–‡æœ¬å—]")
        sql = text(f"""
            SELECT * FROM {schema}.chunks 
            WHERE text ILIKE :pattern
            LIMIT 10
        """)

        with engine.connect() as conn:
            df = pd.read_sql(sql, conn, params={"pattern": search_pattern})

        if not df.empty:
            # æ”¶é›†æ‰€æœ‰å®ä½“IDè¿›è¡ŒåæŸ¥
            all_ent_ids = []
            for x in df['entity_ids']:
                if isinstance(x, list):
                    all_ent_ids.extend(x)
                elif isinstance(x, str):
                    all_ent_ids.extend(json.loads(x))

            ent_name_map = _batch_fetch_entity_names(engine, schema, all_ent_ids)

            for _, row in df.iterrows():
                print(f"  ğŸ“„ ID: {row['chunk_id']}")
                print(f"     æ¥æºæ–‡æ¡£: {row['source_document_name']}")

                # åŒ…å«å®ä½“
                e_ids = row['entity_ids']
                if isinstance(e_ids, str): e_ids = json.loads(e_ids)
                if e_ids:
                    e_names = [ent_name_map.get(eid, eid) for eid in e_ids]
                    # é™åˆ¶æ˜¾ç¤ºæ•°é‡
                    display_names = e_names[:10]
                    suffix = f"... (+{len(e_names) - 10} more)" if len(e_names) > 10 else ""
                    print(f"     ğŸ§© åŒ…å«å®ä½“ ({len(e_ids)}): {', '.join(display_names)} {suffix}")

                # é«˜äº®å†…å®¹
                content = row['text']
                idx = content.lower().find(query_str.lower())
                start = max(0, idx - 60)
                end = min(len(content), idx + len(query_str) + 100)
                preview = content[start:end].replace('\n', ' ')
                print(f"     ğŸ” ä¸Šä¸‹æ–‡: \"...{preview}...\"")
                print("    " + "-" * 60)
            print(f"  âœ… æ‰¾åˆ° {len(df)} ä¸ªæ–‡æœ¬å—ã€‚")
        else:
            print("  (æœªæ‰¾åˆ°ç›¸å…³æ–‡æœ¬å—)")

    print("\nğŸ æœç´¢å®Œæˆã€‚")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="TGS_RAG çŸ¥è¯†åº“é«˜çº§æœç´¢å·¥å…·")
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤: list, search')

    # å‘½ä»¤ 1: list
    parser_list = subparsers.add_parser('list', help='åˆ—å‡ºæ‰€æœ‰çŸ¥è¯†åº“')
    parser_list.add_argument("--config", type=str, default="config.yaml", help="é…ç½®æ–‡ä»¶è·¯å¾„")

    # å‘½ä»¤ 2: search
    parser_search = subparsers.add_parser('search', help='åœ¨æŒ‡å®šçŸ¥è¯†åº“ä¸­æœç´¢')
    parser_search.add_argument("query", type=str, help="æœç´¢å…³é”®è¯")
    parser_search.add_argument("--scope", type=str, default="all", help="æœç´¢èŒƒå›´ (entities, relations, chunks)")
    parser_search.add_argument("--config", type=str, default="config.yaml", help="é…ç½®æ–‡ä»¶è·¯å¾„")

    args = parser.parse_args()

    if args.command == 'list':
        list_knowledge_bases(args.config)
    elif args.command == 'search':
        scope_list = ['entities', 'relations', 'chunks'] if args.scope == 'all' else [s.strip() for s in
                                                                                      args.scope.split(',')]
        search_knowledge_base(args.query, scope_list, args.config)
    else:
        parser.print_help()