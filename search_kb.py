import argparse
import yaml
import pandas as pd
import json
from sqlalchemy import text
from db_utils import DBManager
import sys
from psycopg2.extras import execute_values


def list_knowledge_bases(config_path='config.yaml'):
    """åˆ—å‡ºå½“å‰æ•°æ®åº“ä¸­çš„æ‰€æœ‰çŸ¥è¯†åº“ (Schemas)"""
    print(f"ğŸ” æ­£åœ¨è¯»å–æ•°æ®åº“é…ç½®: {config_path}...")
    try:
        # ä¸´æ—¶åˆå§‹åŒ–ä»¥è·å–è¿æ¥å¼•æ“
        db = DBManager(config_path)
        engine = db.get_engine()

        # æŸ¥è¯¢æ‰€æœ‰éç³»ç»Ÿ Schema
        sql = text("""
            SELECT schema_name 
            FROM information_schema.schemata 
            WHERE schema_name NOT IN ('information_schema', 'public') 
              AND schema_name NOT LIKE 'pg_%'
        """)

        with engine.connect() as conn:
            result = conn.execute(sql)
            schemas = [row[0] for row in result]

        print("\nğŸ“š å½“å‰å­˜åœ¨çš„çŸ¥è¯†åº“ (Rag Spaces):")
        print("=" * 40)
        if schemas:
            for s in schemas:
                print(f"  â€¢ {s}")
        else:
            print("  (æš‚æ— è‡ªå®šä¹‰çŸ¥è¯†åº“)")
        print("=" * 40)

    except Exception as e:
        print(f"âŒ è·å–çŸ¥è¯†åº“åˆ—è¡¨å¤±è´¥: {e}")


def delete_document(doc_name, config_path='config.yaml'):
    """åˆ é™¤æŒ‡å®šæ–‡æ¡£åŠå…¶å…³è”çš„æ‰€æœ‰æ•°æ®"""
    print(f"ğŸ” æ­£åœ¨åˆå§‹åŒ–æ•°æ®åº“è¿æ¥...")
    try:
        db = DBManager(config_path)
        # ä½¿ç”¨ psycopg2 åŸç”Ÿè¿æ¥ä»¥è·å¾—æ›´å¥½çš„äº‹åŠ¡æ§åˆ¶å’Œ execute_values æ”¯æŒ
        conn = db.get_conn()
        conn.autocommit = False  # å¼€å¯äº‹åŠ¡
        schema = db.schema
    except Exception as e:
        print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        return

    print(f"ğŸ—‘ï¸  å‡†å¤‡åˆ é™¤æ–‡æ¡£: '{doc_name}' (çŸ¥è¯†åº“: {schema})")
    print("=" * 60)

    try:
        with conn.cursor() as cur:
            # 1. æŸ¥æ‰¾å±äºè¯¥æ–‡æ¡£çš„æ‰€æœ‰ Chunk IDs
            print(f"1ï¸âƒ£  æ­£åœ¨æŸ¥æ‰¾ç›¸å…³ Chunks...")
            cur.execute(f"SELECT chunk_id FROM {schema}.chunks WHERE source_document_name = %s", (doc_name,))
            rows = cur.fetchall()

            if not rows:
                print(f"âš ï¸  æœªæ‰¾åˆ°åä¸º '{doc_name}' çš„æ–‡æ¡£æ•°æ®ã€‚è¯·æ£€æŸ¥æ–‡ä»¶åï¼ˆåŒ…å«æ‰©å±•åï¼‰ã€‚")
                return

            deleted_chunk_ids = set(row[0] for row in rows)
            deleted_chunk_ids_list = list(deleted_chunk_ids)  # ç”¨äº SQL å‚æ•°
            print(f"   âœ… æ‰¾åˆ° {len(deleted_chunk_ids)} ä¸ª Chunksã€‚")

            # å®šä¹‰å¤„ç†å®ä½“å’Œå…³ç³»çš„é€šç”¨å‡½æ•°
            def process_table(table_name, id_col, name_col):
                print(f"\n2ï¸âƒ£  æ­£åœ¨æ£€æŸ¥å—å½±å“çš„ {table_name}...")

                # ä½¿ç”¨ JSONB æ“ä½œç¬¦ ?| æŸ¥æ‰¾åŒ…å«ä»»æ„å¾…åˆ é™¤ Chunk ID çš„è¡Œ
                # è¿™æ¯” Python å¾ªç¯è¿‡æ»¤å…¨è¡¨è¦é«˜æ•ˆå¾—å¤š
                query = f"""
                    SELECT {id_col}, {name_col}, source_chunk_ids 
                    FROM {schema}.{table_name} 
                    WHERE source_chunk_ids ?| %s
                """
                cur.execute(query, (deleted_chunk_ids_list,))
                candidates = cur.fetchall()

                if not candidates:
                    print(f"   â„¹ï¸  æ²¡æœ‰ {table_name} å—åˆ°å½±å“ã€‚")
                    return

                to_update = []  # [(id, new_json_str), ...]
                to_delete = []  # [id, ...]

                for row in candidates:
                    row_id, row_name, src_chunks = row

                    # è¿‡æ»¤ chunk list
                    if isinstance(src_chunks, str): src_chunks = json.loads(src_chunks)
                    if not isinstance(src_chunks, list): src_chunks = []

                    new_chunks = [c for c in src_chunks if c not in deleted_chunk_ids]

                    if not new_chunks:
                        # å¦‚æœæ¥æºåˆ—è¡¨ç©ºäº†ï¼Œè¯´æ˜è¯¥å®ä½“/å…³ç³»ä»…æ¥æºäºè¢«åˆ é™¤çš„æ–‡æ¡£ -> åˆ é™¤
                        to_delete.append(row_id)
                    elif len(new_chunks) != len(src_chunks):
                        # å¦åˆ™ -> æ›´æ–°
                        to_update.append((row_id, json.dumps(new_chunks)))

                # æ‰§è¡Œæ›´æ–°
                if to_update:
                    print(f"   ğŸ“ æ›´æ–° {len(to_update)} ä¸ª {table_name} (ç§»é™¤å¼•ç”¨æº)...")
                    update_sql = f"""
                        UPDATE {schema}.{table_name} AS t
                        SET source_chunk_ids = v.new_ids::jsonb,
                            -- å¯é€‰ï¼šå¦‚æœ frequency æ˜¯åŸºäº chunk è®¡æ•°çš„ï¼Œè¿™é‡Œå¯èƒ½éœ€è¦é€’å‡ï¼Œ
                            -- ä½†ç”±äº freq é€»è¾‘è¾ƒå¤æ‚ï¼Œæš‚åªå¤„ç†å¼•ç”¨ IDï¼Œä¿è¯å›¾è°±è¿é€šæ€§æ­£ç¡®ã€‚
                            frequency = GREATEST(1, cardinality(ARRAY(SELECT jsonb_array_elements_text(v.new_ids::jsonb))))
                        FROM (VALUES %s) AS v(id, new_ids)
                        WHERE t.{id_col} = v.id
                    """
                    execute_values(cur, update_sql, to_update)

                # æ‰§è¡Œåˆ é™¤
                if to_delete:
                    print(f"   ğŸ—‘ï¸  åˆ é™¤ {len(to_delete)} ä¸ª {table_name} (å¼•ç”¨æºå½’é›¶)...")
                    cur.execute(f"DELETE FROM {schema}.{table_name} WHERE {id_col} = ANY(%s)", (to_delete,))

            # 2. å¤„ç†å®ä½“
            process_table('entities', 'entity_id', 'entity_name')

            # 3. å¤„ç†å…³ç³»
            process_table('relationships', 'relation_id', 'relation_id')  # relation_id æ—¢æ˜¯IDä¹Ÿæ˜¯å ä½å

            # 4. åˆ é™¤ Chunks
            print(f"\n4ï¸âƒ£  æ­£åœ¨ç‰©ç†åˆ é™¤ Chunks...")
            cur.execute(f"DELETE FROM {schema}.chunks WHERE chunk_id = ANY(%s)", (deleted_chunk_ids_list,))

            conn.commit()
            print(f"\nâœ… åˆ é™¤æ“ä½œæˆåŠŸå®Œæˆï¼æ–‡æ¡£ '{doc_name}' å·²ä»çŸ¥è¯†åº“ç§»é™¤ã€‚")

    except Exception as e:
        conn.rollback()
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯ï¼Œå·²å›æ»šæ‰€æœ‰æ“ä½œ: {e}")
    finally:
        conn.close()


def _batch_fetch_chunks(engine, schema, chunk_ids):
    """æ‰¹é‡è·å– Chunk è¯¦æƒ…"""
    if not chunk_ids: return {}
    ids = tuple(set(chunk_ids))
    if not ids: return {}
    ids_sql = str(ids)
    if len(ids) == 1: ids_sql = f"('{ids[0]}')"
    sql = text(f"SELECT chunk_id, source_document_name, text FROM {schema}.chunks WHERE chunk_id IN {ids_sql}")
    chunk_map = {}
    try:
        with engine.connect() as conn:
            df = pd.read_sql(sql, conn)
            for _, row in df.iterrows():
                chunk_map[row['chunk_id']] = {'doc': row['source_document_name'],
                                              'preview': row['text'][:50].replace('\n', ' ') + "..."}
    except Exception:
        pass
    return chunk_map


def _batch_fetch_entity_names(engine, schema, entity_ids):
    """æ‰¹é‡è·å–å®ä½“åç§°"""
    if not entity_ids: return {}
    ids = tuple(set(entity_ids))
    if not ids: return {}
    ids_sql = str(ids)
    if len(ids) == 1: ids_sql = f"('{ids[0]}')"
    sql = text(f"SELECT entity_id, entity_name FROM {schema}.entities WHERE entity_id IN {ids_sql}")
    name_map = {}
    try:
        with engine.connect() as conn:
            df = pd.read_sql(sql, conn)
            for _, row in df.iterrows():
                name_map[row['entity_id']] = row['entity_name']
    except Exception:
        pass
    return name_map


def search_knowledge_base(query_str, scope, config_path='config.yaml'):
    try:
        db = DBManager(config_path)
        engine = db.get_engine()
        schema = db.schema
    except Exception as e:
        print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        return

    print(f"ğŸ” æœç´¢ç›®æ ‡: '{query_str}' | çŸ¥è¯†åº“: {db.rag_space} ({schema})")
    print("=" * 80)
    search_pattern = f"%{query_str}%"

    if 'entities' in scope:
        print(f"\n[ğŸ§© Entities / å®ä½“]")
        sql = text(
            f"SELECT * FROM {schema}.entities WHERE entity_name ILIKE :pattern OR description ILIKE :pattern LIMIT 20")
        with engine.connect() as conn:
            df = pd.read_sql(sql, conn, params={"pattern": search_pattern})
        if not df.empty:
            all_chunk_ids = []
            for x in df['source_chunk_ids']:
                if isinstance(x, list):
                    all_chunk_ids.extend(x)
                elif isinstance(x, str):
                    all_chunk_ids.extend(json.loads(x))
            chunk_map = _batch_fetch_chunks(engine, schema, all_chunk_ids)
            for _, row in df.iterrows():
                print(f"  ğŸ“ {row['entity_name']} (Type: {row['entity_type']})")
                print(f"     ID: {row['entity_id']}")
                print(f"     ğŸ“Š æƒé‡(Freq): {row['frequency']} | è¿æ¥åº¦(Degree): {row['degree']}")
                desc = row['description']
                if len(desc) > 100: desc = desc[:100] + "..."
                print(f"     ğŸ“ æè¿°: {desc}")
                src_ids = row['source_chunk_ids']
                if isinstance(src_ids, str): src_ids = json.loads(src_ids)
                if src_ids:
                    print(f"     ğŸ“„ æ¥æº ({len(src_ids)} Chunks):")
                    docs = {}
                    for cid in src_ids:
                        info = chunk_map.get(cid, {'doc': 'Unknown', 'preview': '?'})
                        dname = info['doc']
                        if dname not in docs: docs[dname] = []
                        docs[dname].append(cid)
                    for dname, cids in docs.items():
                        print(f"       - æ–‡æ¡£: {dname}\n         Chunks: {', '.join(cids)}")
                print("    " + "-" * 60)
            print(f"  âœ… æ‰¾åˆ° {len(df)} ä¸ªç›¸å…³å®ä½“ã€‚")
        else:
            print("  (æœªæ‰¾åˆ°ç›¸å…³å®ä½“)")

    if 'relations' in scope:
        print(f"\n[ğŸ”— Relationships / å…³ç³»]")
        sql = text(
            f"SELECT * FROM {schema}.relationships WHERE source_name ILIKE :pattern OR target_name ILIKE :pattern OR keywords ILIKE :pattern LIMIT 20")
        with engine.connect() as conn:
            df = pd.read_sql(sql, conn, params={"pattern": search_pattern})
        if not df.empty:
            all_chunk_ids = []
            for x in df['source_chunk_ids']:
                if isinstance(x, list):
                    all_chunk_ids.extend(x)
                elif isinstance(x, str):
                    all_chunk_ids.extend(json.loads(x))
            chunk_map = _batch_fetch_chunks(engine, schema, all_chunk_ids)
            for _, row in df.iterrows():
                print(f"  ğŸ”— {row['source_name']} -> {row['target_name']}")
                print(f"     ID: {row['relation_id']}")
                print(f"     ğŸ·ï¸  å…³é”®è¯: {row['keywords']}")
                print(f"     ğŸ“Š æƒé‡(Freq): {row['frequency']} | è¿æ¥åº¦(Degree): {row['degree']}")
                print(f"     ğŸ“ æè¿°: {row['description'][:100]}..." if len(
                    str(row['description'])) > 100 else f"     ğŸ“ æè¿°: {row['description']}")
                src_ids = row['source_chunk_ids']
                if isinstance(src_ids, str): src_ids = json.loads(src_ids)
                if src_ids:
                    print(f"     ğŸ“„ æ¥æº ({len(src_ids)} Chunks):")
                    docs = {}
                    for cid in src_ids:
                        info = chunk_map.get(cid, {'doc': 'Unknown'})
                        dname = info['doc']
                        if dname not in docs: docs[dname] = []
                        docs[dname].append(cid)
                    for dname, cids in docs.items():
                        print(f"       - {dname}: {', '.join(cids)}")
                print("    " + "-" * 60)
            print(f"  âœ… æ‰¾åˆ° {len(df)} ä¸ªç›¸å…³å…³ç³»ã€‚")
        else:
            print("  (æœªæ‰¾åˆ°ç›¸å…³å…³ç³»)")

    if 'chunks' in scope:
        print(f"\n[ğŸ“„ Chunks / æ–‡æœ¬å—]")
        sql = text(f"SELECT * FROM {schema}.chunks WHERE text ILIKE :pattern LIMIT 10")
        with engine.connect() as conn:
            df = pd.read_sql(sql, conn, params={"pattern": search_pattern})
        if not df.empty:
            all_ent_ids = []
            for x in df['entity_ids']:
                if isinstance(x, list):
                    all_ent_ids.extend(x)
                elif isinstance(x, str):
                    all_ent_ids.extend(json.loads(x))
            ent_name_map = _batch_fetch_entity_names(engine, schema, all_ent_ids)
            for _, row in df.iterrows():
                print(f"  ğŸ“„ ID: {row['chunk_id']}")
                print(f"     æ¥æºæ–‡æ¡£: {row['source_document_name']}")
                e_ids = row['entity_ids']
                if isinstance(e_ids, str): e_ids = json.loads(e_ids)
                if e_ids:
                    e_names = [ent_name_map.get(eid, eid) for eid in e_ids]
                    display_names = e_names[:10]
                    suffix = f"... (+{len(e_names) - 10} more)" if len(e_names) > 10 else ""
                    print(f"     ğŸ§© åŒ…å«å®ä½“ ({len(e_ids)}): {', '.join(display_names)} {suffix}")
                content = row['text']
                idx = content.lower().find(query_str.lower())
                start = max(0, idx - 60)
                end = min(len(content), idx + len(query_str) + 100)
                preview = content[start:end].replace('\n', ' ')
                print(f"     ğŸ” ä¸Šä¸‹æ–‡: \"...{preview}...\"")
                print("    " + "-" * 60)
            print(f"  âœ… æ‰¾åˆ° {len(df)} ä¸ªæ–‡æœ¬å—ã€‚")
        else:
            print("  (æœªæ‰¾åˆ°ç›¸å…³æ–‡æœ¬å—)")
    print("\nğŸ æœç´¢å®Œæˆã€‚")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="TGS_RAG çŸ¥è¯†åº“ç®¡ç†ä¸æœç´¢å·¥å…·")
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')

    # Command: list
    parser_list = subparsers.add_parser('list', help='åˆ—å‡ºæ‰€æœ‰çŸ¥è¯†åº“')
    parser_list.add_argument("--config", type=str, default="config.yaml", help="é…ç½®æ–‡ä»¶è·¯å¾„")

    # Command: search
    parser_search = subparsers.add_parser('search', help='åœ¨æŒ‡å®šçŸ¥è¯†åº“ä¸­æœç´¢')
    parser_search.add_argument("query", type=str, help="æœç´¢å…³é”®è¯")
    parser_search.add_argument("--scope", type=str, default="all", help="æœç´¢èŒƒå›´ (entities, relations, chunks)")
    parser_search.add_argument("--config", type=str, default="config.yaml", help="é…ç½®æ–‡ä»¶è·¯å¾„")

    # Command: delete_doc
    parser_del = subparsers.add_parser('delete_doc', help='åˆ é™¤æŒ‡å®šæ–‡æ¡£åŠå…¶æ‰€æœ‰å…³è”æ•°æ®')
    parser_del.add_argument("doc_name", type=str, help="è¦åˆ é™¤çš„æ–‡æ¡£å…¨å (ä¾‹å¦‚: '71 (film).md')")
    parser_del.add_argument("--config", type=str, default="config.yaml", help="é…ç½®æ–‡ä»¶è·¯å¾„")

    args = parser.parse_args()

    if args.command == 'list':
        list_knowledge_bases(args.config)
    elif args.command == 'search':
        scope_list = ['entities', 'relations', 'chunks'] if args.scope == 'all' else [s.strip() for s in
                                                                                      args.scope.split(',')]
        search_knowledge_base(args.query, scope_list, args.config)
    elif args.command == 'delete_doc':
        delete_document(args.doc_name, args.config)
    else:
        parser.print_help()