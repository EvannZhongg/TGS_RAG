# app.py
import streamlit as st
import pandas as pd
import json
import time
import yaml
from sqlalchemy import text

# å¼•å…¥æ–°çš„æ£€ç´¢å™¨
from retriever_db import PathSBERetriever
from streamlit_agraph import agraph, Node, Edge, Config

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="å¯è§†åŒ–è·¯å¾„å¢å¼ºRAG (Path-SBEA)", layout="wide")


# --- ç¼“å­˜å’ŒåŠ è½½ ---
@st.cache_resource
def load_retriever():
    # æ–°ç‰ˆåˆå§‹åŒ–ä¸éœ€è¦åŠ è½½å¤§é‡æ•°æ®ï¼Œéå¸¸å¿«
    return PathSBERetriever()


# åŠ è½½é…ç½®
try:
    with open('config.yaml', 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    VIZ_CONFIG = config.get('Visualization', {})
except:
    VIZ_CONFIG = {}

MAX_PATHS_TO_RENDER_DEFAULT = VIZ_CONFIG.get('max_paths_to_render', 100)

retriever = load_retriever()


# --- æ–°å¢ï¼šæ•°æ®æŒ‰éœ€æŠ“å–è¾…åŠ©å‡½æ•° ---
def fetch_realtime_data(retriever_instance, entity_ids=None, chunk_ids=None):
    """
    ç”±äºæ£€ç´¢å™¨ä¸å†å°†æ‰€æœ‰æ•°æ®åŠ è½½åˆ°å†…å­˜ï¼Œå¯è§†åŒ–å‰éœ€è¦æ ¹æ®IDå»æ•°æ®åº“æŠ“å–è¯¦æƒ…ã€‚
    """
    local_entity_map = {}
    local_chunk_map = {}

    schema = retriever_instance.db.schema
    engine = retriever_instance.db.get_engine()

    # 1. æŠ“å–å®ä½“è¯¦æƒ…
    if entity_ids:
        # å¤ç”¨ retriever å†…éƒ¨çš„æŠ“å–é€»è¾‘
        local_entity_map, _ = retriever_instance._fetch_local_graph_data(entity_ids)

    # 2. æŠ“å–æ–‡æœ¬å—è¯¦æƒ…
    if chunk_ids:
        ids_tuple = tuple(chunk_ids)
        if ids_tuple:
            ids_sql = str(ids_tuple)
            if len(ids_tuple) == 1: ids_sql = f"('{ids_tuple[0]}')"

            sql = f"SELECT chunk_id, text, source_document_name FROM {schema}.chunks WHERE chunk_id IN {ids_sql}"
            try:
                with engine.connect() as conn:
                    df = pd.read_sql(sql, conn)
                for _, row in df.iterrows():
                    local_chunk_map[row['chunk_id']] = {
                        'text': row['text'],
                        'source_document_name': row['source_document_name']
                    }
            except Exception as e:
                st.error(f"Error fetching chunks: {e}")

    return local_entity_map, local_chunk_map


# --- å¯è§†åŒ–è¾…åŠ©å‡½æ•° (å¾®è°ƒ) ---
def build_graph_viz(paths, entity_map, seed_ids, bridged_path_ids, highlight_path_ids=None, key=None):
    """
    æ„å»ºå¹¶æ¸²æŸ“å›¾è°±ã€‚
    ä¿®å¤è¯´æ˜ï¼šé€šè¿‡ä¿®æ”¹ config å¯¹è±¡æ¥å®ç°å”¯ä¸€æ€§ï¼Œè§£å†³ streamlit-agraph ä¸æ”¯æŒ key å‚æ•°çš„é—®é¢˜ã€‚
    """
    nodes, edges = [], []
    added_nodes, added_edges = set(), set()

    bridged_nodes = {eid for p in bridged_path_ids for eid in p}
    highlight_nodes = {eid for p in highlight_path_ids for eid in p} if highlight_path_ids else set()

    for path in paths:
        for i, entity_id in enumerate(path):
            if entity_id not in added_nodes:
                entity_info = entity_map.get(entity_id, {})
                entity_name = entity_info.get('entity_name', entity_id)

                color = "#6495ED"
                size = 15

                if entity_id in seed_ids:
                    color = "#3CB371"
                    size = 20
                elif entity_id in highlight_nodes:
                    color = "#FF4500"
                    size = 20
                elif entity_id in bridged_nodes:
                    color = "#FFA500"

                added_nodes.add(entity_id)
                nodes.append(Node(id=entity_id, label=entity_name, color=color, size=size))

            if len(path) > 1 and i > 0:
                edge = tuple(sorted((path[i - 1], entity_id)))
                if edge not in added_edges:
                    edges.append(Edge(source=edge[0], target=edge[1]))
                    added_edges.add(edge)

    config = Config(width='100%', height=600, directed=False, physics=True, hierarchical=False,
                    solver='forceAtlas2Based',
                    forceAtlas2Based={"gravitationalConstant": -50, "centralGravity": 0.005,
                                      "springLength": 100, "springConstant": 0.18})

    # <--- æ ¸å¿ƒä¿®å¤ï¼šå°† key æ³¨å…¥åˆ° config ä¸­ --->
    # è¿™æ · Streamlit ä¼šæ£€æµ‹åˆ° config å¯¹è±¡å‘ç”Ÿäº†å˜åŒ–ï¼Œä»è€Œä¸ºå›¾è¡¨ç”Ÿæˆå”¯ä¸€çš„ Element ID
    # è€Œ agraph å‡½æ•°æœ¬èº«ä¸éœ€è¦æ¥æ”¶ key å‚æ•°ï¼Œé¿å…äº† TypeError
    if key is not None:
        config.__dict__['hack_unique_key'] = key

    # ç§»é™¤ key=keyï¼Œåªä¼  config
    return agraph(nodes=nodes, edges=edges, config=config)


# --- ä¸»ç•Œé¢ ---
st.title("ğŸ’¡ å¯è§†åŒ–è·¯å¾„å¢å¼ºååŒæ£€ç´¢ (Path-SBEA) - DBç‰ˆ")
st.sidebar.title("ğŸ” æ§åˆ¶é¢æ¿")

# Sidebar controls
query = st.sidebar.text_area("1. è¯·è¾“å…¥æ‚¨çš„é—®é¢˜:", value="RoHSæŒ‡ä»¤å’Œå³°å€¼æ­£å‘ç”µæµæœ‰ä»€ä¹ˆå…³ç³»?", height=100)
top_k_chunks = st.sidebar.slider("2. Top K Chunks", 1, 10, 5)
top_k_paths = st.sidebar.slider("3. Top K Paths", 1, 10, 5)
max_paths_to_render = st.sidebar.slider("5. å¯è§†åŒ–æœ€å¤§è·¯å¾„æ•°", 10, 500, MAX_PATHS_TO_RENDER_DEFAULT)
answering_mode = st.sidebar.selectbox(
    "4. æœ€ç»ˆç­”æ¡ˆç”Ÿæˆæ¨¡å¼",
    options=["full_context", "chunks_only", "paths_only"],
    index=0,
    format_func=lambda x: {"full_context": "å®Œæ•´ä¸Šä¸‹æ–‡", "chunks_only": "ä»…æ–‡æœ¬", "paths_only": "ä»…å›¾è°±"}[x],
    help="é€‰æ‹©å°†å“ªç§ç±»å‹çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æäº¤ç»™LLMä»¥ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚"
)

if st.sidebar.button("ğŸš€ æ‰§è¡Œæ£€ç´¢ä¸å›ç­”"):
    if not query:
        st.warning("è¯·è¾“å…¥é—®é¢˜ï¼")
    else:
        for key in ['results', 'diagnostics', 'final_answer_stream', 'viz_data_cache']:
            if key in st.session_state:
                del st.session_state[key]

        with st.spinner("æ‰§è¡Œä¸­... (1/2) æ£€ç´¢ç›¸å…³ä¿¡æ¯..."):
            # æ‰§è¡Œæœç´¢
            results, diagnostics = retriever.search(query, top_k_chunks, top_k_paths)
            st.session_state.results = results
            st.session_state.diagnostics = diagnostics

            # --- é¢„åŠ è½½å¯è§†åŒ–æ•°æ® ---
            # æ”¶é›†æ‰€æœ‰éœ€è¦å±•ç¤ºçš„ Entity ID å’Œ Chunk ID
            all_viz_entity_ids = set()

            # 1. ç§å­å®ä½“
            seed_entities = results.get('seed_entities', [])
            all_viz_entity_ids.update([s['id'] for s in seed_entities])

            # 2. æ‰€æœ‰è·¯å¾„ä¸Šçš„èŠ‚ç‚¹ (åŒ…æ‹¬è¢«æ¡¥æ¥çš„)
            all_paths = results.get('all_paths', [])
            for p in all_paths:
                all_viz_entity_ids.update(p['path'])

            # 3. å€™é€‰ Chunk IDs
            candidate_chunks = results.get('candidate_chunks', [])
            all_viz_chunk_ids = {c['id'] for c in candidate_chunks}

            # æ‰¹é‡æŠ“å–è¯¦æƒ…å¹¶ç¼“å­˜
            ent_map, chunk_map = fetch_realtime_data(retriever, all_viz_entity_ids, all_viz_chunk_ids)
            st.session_state.viz_data_cache = {
                'entity_map': ent_map,
                'chunk_map': chunk_map
            }

        with st.spinner("æ‰§è¡Œä¸­... (2/2) ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ..."):
            answer_gen_start_time = time.time()
            answer_stream = retriever.generate_answer(
                query,
                st.session_state.results['top_chunks'],
                st.session_state.results['top_paths'],
                answering_mode
            )
            st.session_state.final_answer_stream = answer_stream
            answer_gen_time = time.time() - answer_gen_start_time
            st.session_state.diagnostics['time_answer_generation'] = f"{answer_gen_time:.2f}s"

# --- ä¾§è¾¹æ ç›‘æ§ä¿¡æ¯å±•ç¤º ---
if 'diagnostics' in st.session_state:
    with st.sidebar.expander("ğŸ¤– LLM åˆå§‹å®ä½“æŠ½å–", expanded=True):
        extracted = st.session_state.diagnostics.get('llm_extraction', {})
        st.json(extracted.get('entities', ["æ— "]))
        usage = extracted.get('usage')
        if usage:
            st.write(f"**Tokens:** {usage.total_tokens}")

# --- ä¸»é¡µé¢ç»“æœå±•ç¤º ---
if 'results' in st.session_state and 'viz_data_cache' in st.session_state:
    results = st.session_state.results
    viz_cache = st.session_state.viz_data_cache
    entity_map_cache = viz_cache['entity_map']
    chunk_map_cache = viz_cache['chunk_map']

    # æœ€ç»ˆç­”æ¡ˆåŒº
    if 'final_answer_stream' in st.session_state:
        st.markdown("---")
        st.subheader("ğŸ¤– æœ€ç»ˆç”Ÿæˆç­”æ¡ˆ")
        st.write_stream(st.session_state.final_answer_stream)

    st.markdown("---")
    st.header("ğŸ” æ£€ç´¢è¿‡ç¨‹è¯¦è§£")

    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“Š æœ€ç»ˆæ£€ç´¢ç»“æœ", "ğŸ•¸ï¸ å›¾è°±å¯è§†åŒ–æ¼”è¿›", "ğŸ“š æ–‡æœ¬å—ç­›é€‰è¿‡ç¨‹", "â±ï¸ æ€§èƒ½ç›‘æ§"])

    # Tab 1: æœ€ç»ˆç»“æœ
    with tab1:
        col1, col2 = st.columns(2)
        with col1:
            st.subheader("ğŸ† Top K æ¨ç†è·¯å¾„")
            for i, path in enumerate(results['top_paths']):
                with st.expander(f"è·¯å¾„ #{i + 1} | Score: {path['score']:.3f}", expanded=False):
                    st.success(f"`{path['path_readable']}`")
                    st.caption(f"ç”ŸæˆåŸå› : {path['reason']}")
                    # è¿™é‡Œçš„ segments å·²ç»åœ¨ retriever.get_path_details ä¸­å¡«å……å¥½äº†ï¼Œæ— éœ€é¢å¤–å¤„ç†
                    for segment in path.get('segments', []):
                        st.markdown(f"**- {segment['source']}**")
                        st.caption(f"  {segment['source_desc']}")
                        st.markdown(f"  â”” `å…³ç³»`: {segment['keywords']} - *{segment['description']}*")
                        st.markdown(f"**- {segment['target']}**")
                        st.caption(f"  {segment['target_desc']}")
                    if path.get('endorsing_bridges'):
                        st.markdown("---")
                        st.markdown("**ç”±ä»¥ä¸‹è¯æ®è¡¥å…¨æ”¯æŒ:**")
                        for bridge_readable in path['endorsing_bridges']:
                            st.info(f"`{bridge_readable}`")
        with col2:
            st.subheader("ğŸ“š Top K æ–‡æœ¬è¯æ®")
            for i, chunk in enumerate(results['top_chunks']):
                with st.expander(f"æ–‡æœ¬ #{i + 1} | Score: {chunk['score']:.3f}", expanded=False):
                    st.info(f"**{chunk['name']}**")
                    st.caption(chunk['content'])
                    st.caption(f"è¯„åˆ†æ„æˆ: {chunk['reason']}")

    # Tab 2: å›¾è°±å¯è§†åŒ–
    with tab2:
        st.subheader("å›¾è°±æ„å»ºä¸ç­›é€‰çš„ä¸‰ä¸ªé˜¶æ®µ")
        st.markdown("""
        - **<font color='#3CB371'>ç»¿è‰²èŠ‚ç‚¹</font>**: èµ·ç‚¹å®ä½“ (LLMæŠ½å–+å‘é‡æ£€ç´¢)ã€‚
        - **<font color='#6495ED'>è“è‰²èŠ‚ç‚¹</font>**: é€šè¿‡BFSä»èµ·ç‚¹æ‰©å±•çš„å®ä½“ã€‚
        - **<font color='#FFA500'>æ©™è‰²èŠ‚ç‚¹</font>**: é€šè¿‡â€œæ¡¥æ¥â€æ–‡æœ¬è¯æ®ä¸­çš„å­¤ç«‹å®ä½“è€Œè¡¥å…¨çš„è·¯å¾„èŠ‚ç‚¹ã€‚
        - **<font color='#FF4500'>çº¢è‰²èŠ‚ç‚¹</font>**: æœ€ç»ˆè¢«é€‰å…¥Top-Kæ¨ç†è·¯å¾„çš„æ ¸å¿ƒå®ä½“ã€‚
        """, unsafe_allow_html=True)

        seed_entities = results.get('seed_entities', [])
        all_paths = results.get('all_paths', [])  # è¿™é‡Œçš„ item åªæœ‰ path (id list), score ç­‰
        bridged_paths = results.get('bridged_paths', [])
        top_paths_info = results.get('top_paths', [])

        seed_ids = {s['id'] for s in seed_entities}
        bridged_path_ids = [p['path'] for p in bridged_paths]
        top_paths_ids = [p['entity_ids'] for p in top_paths_info]  # get_path_details è¿”å›äº† entity_ids

        with st.expander("1. èµ·ç‚¹å®ä½“å›¾è°±", expanded=True):
            if seed_ids:
                build_graph_viz(
                    paths=[[sid] for sid in seed_ids],
                    entity_map=entity_map_cache,
                    seed_ids=seed_ids,
                    bridged_path_ids=[],
                    key="viz_seed_graph"  # <--- æ–°å¢å”¯ä¸€ Key
                )
            else:
                st.info("æœªèƒ½æ‰¾åˆ°èµ·ç‚¹å®ä½“ã€‚")

        with st.expander("2. æ‰©å±•ä¸æ¡¥æ¥å…¨å›¾", expanded=False):
            if all_paths:
                bridged_path_ids_set = {tuple(p.get('path', [])) for p in bridged_paths}
                paths_to_render = [p['path'] for p in all_paths if tuple(p['path']) in bridged_path_ids_set]

                # è¡¥å……éæ¡¥æ¥è·¯å¾„ç›´åˆ°ä¸Šé™
                remaining_slots = max_paths_to_render - len(paths_to_render)
                if remaining_slots > 0:
                    non_bridged = [p['path'] for p in all_paths if tuple(p['path']) not in bridged_path_ids_set]
                    paths_to_render.extend(non_bridged[:remaining_slots])

                if len(all_paths) > max_paths_to_render:
                    st.info(f"ä¸ºä¿æŒæµç•…ï¼Œä»…æ˜¾ç¤º {len(paths_to_render)}/{len(all_paths)} æ¡è·¯å¾„ã€‚")

                build_graph_viz(
                    paths=paths_to_render,
                    entity_map=entity_map_cache,
                    seed_ids=seed_ids,
                    bridged_path_ids=bridged_path_ids,
                    key="viz_full_graph"  # <--- æ–°å¢å”¯ä¸€ Key
                )
            else:
                st.info("æœªèƒ½é€šè¿‡BFSæ‰©å±•å›¾è°±ã€‚")

        with st.expander("3. æœ€ç»ˆé€‰å®šè·¯å¾„å›¾", expanded=False):
            if top_paths_ids:
                build_graph_viz(
                    paths=top_paths_ids,
                    entity_map=entity_map_cache,
                    seed_ids=seed_ids,
                    bridged_path_ids=bridged_path_ids,
                    highlight_path_ids=top_paths_ids,
                    key="viz_final_graph"  # <--- æ–°å¢å”¯ä¸€ Key
                )
            else:
                st.info("æœ€ç»ˆæœªèƒ½ç­›é€‰å‡ºä»»ä½•è·¯å¾„ã€‚")

    # Tab 3: æ–‡æœ¬å—è¿‡ç¨‹
    with tab3:
        st.subheader("æ–‡æœ¬å—ä»åˆå§‹æ£€ç´¢åˆ°æœ€ç»ˆæ’åºçš„å…¨è¿‡ç¨‹")

        # 1. è·å–æ•°æ®
        initial_chunks = results.get('initial_chunks', [])  # Retriever å·²ç»å¡«å……å¥½äº†å†…å®¹
        candidate_chunks = results.get('candidate_chunks', [])

        # 2. å‡†å¤‡å€™é€‰æ± æ•°æ® (éœ€è¦ç”¨ cache å¡«å……å†…å®¹)
        display_candidates = []
        for c in candidate_chunks:
            cid = c['id']
            details = chunk_map_cache.get(cid, {})
            display_candidates.append({
                **c,
                'name': f"Chunk from {details.get('source_document_name', 'Unknown')}",
                'content': details.get('text', 'å†…å®¹åŠ è½½å¤±è´¥'),
            })

        # <--- ã€æ–°å¢ã€‘å±•ç¤ºåˆå§‹æ£€ç´¢ (Top-K å‘é‡æ£€ç´¢ç»“æœ) --->
        with st.expander(f"1. åˆå§‹æ£€ç´¢æ–‡æœ¬å— (Vector Search Top-2K: {len(initial_chunks)}ä¸ª)", expanded=False):
            # æ³¨æ„ï¼šinitial_chunks å·²ç»åœ¨ retriever ä¸­é€šè¿‡ get_item_details å¡«å……äº† content
            for chunk in initial_chunks:
                st.info(
                    f"**{chunk['name']}**\n\n"
                    f"{chunk['content'][:200]}..."  # é¢„è§ˆ
                )

        # <--- ã€ä¿®æ”¹ã€‘åºå·é¡ºå»¶ --->
        with st.expander(f"2. å€™é€‰æ± æ–‡æœ¬å— (Initial + Graph Recs: {len(display_candidates)}ä¸ª)", expanded=False):
            sorted_candidates = sorted(display_candidates, key=lambda x: x['final_score'], reverse=True)
            for chunk in sorted_candidates:
                st.warning(
                    f"**{chunk['name']}** (Final Score: {chunk['final_score']:.3f})\n\n"
                    f"*è¯„åˆ†æ„æˆ: {chunk['reason']}*\n\n"
                    f"{chunk['content'][:200]}..."
                )

        with st.expander(f"3. æœ€ç»ˆé€‰å®šæ–‡æœ¬å— (Top {len(results['top_chunks'])} Chunks)", expanded=False):
            for chunk in results['top_chunks']:
                st.success(
                    f"**{chunk['name']}** (Final Score: {chunk['score']:.3f})\n\n{chunk['content']}"
                )

    # Tab 4: æ€§èƒ½
    with tab4:
        st.subheader("â±ï¸ å„é˜¶æ®µæ€§èƒ½æŒ‡æ ‡")
        diagnostics = st.session_state.diagnostics
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("#### è€—æ—¶")
            st.metric(label="æ€»æ£€ç´¢è€—æ—¶", value=diagnostics.get('time_total_retrieval', 'N/A'))
            st.text(f"  - é˜¶æ®µ1 (åˆå§‹æ£€ç´¢): {diagnostics.get('time_stage1_retrieval', 'N/A')}")
            st.text(f"  - é˜¶æ®µ2 (èåˆè¯„åˆ†): {diagnostics.get('time_stage2_fusion', 'N/A')}")
            st.text(f"  - é˜¶æ®µ3 (æ’åº): {diagnostics.get('time_stage3_ranking', 'N/A')}")
            st.metric(label="æœ€ç»ˆç­”æ¡ˆç”Ÿæˆè€—æ—¶", value=diagnostics.get('time_answer_generation', 'N/A'))
        with col2:
            st.markdown("#### Token æ¶ˆè€—")
            usage = diagnostics.get('llm_extraction', {}).get('usage')
            if usage:
                st.metric(label="å®ä½“æŠ½å–æ€» Tokens", value=usage.total_tokens)